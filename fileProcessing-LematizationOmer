package ex;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.io.PrintWriter;
import java.util.*;

import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.select.Elements;

import com.rapidminer.Process;
import com.rapidminer.RapidMiner;
import com.rapidminer.RepositoryProcessLocation;
import com.rapidminer.operator.IOContainer;
import com.rapidminer.operator.OperatorException;
import com.rapidminer.repository.ProcessEntry;
import com.rapidminer.repository.RepositoryException;
import com.rapidminer.repository.RepositoryLocation;
import com.rapidminer.tools.XMLException;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.ling.CoreAnnotations.NamedEntityTagAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.dcoref.CorefChain;
import edu.stanford.nlp.dcoref.CorefCoreAnnotations.CorefChainAnnotation;
import edu.stanford.nlp.hcoref.CorefCoreAnnotations.CorefMentionsAnnotation;
import edu.stanford.nlp.hcoref.data.Mention;
import edu.stanford.nlp.ling.IndexedWord;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.semgraph.SemanticGraph;
import edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.trees.TreeCoreAnnotations;
import edu.stanford.nlp.trees.TreeCoreAnnotations.TreeAnnotation;
import edu.stanford.nlp.util.CoreMap;

import java.io.BufferedReader;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;



public class Ex {
	
	/*
	 *Delete files from 
	 *1) C:\\Users\\hp\\Desktop\\Courses\\Team Project\\TeamProjectDemo1\\TeamDemo1.csv
	 *2) Delete 50 files
	 *3) 
	 */
	private static String fileName = "topic 2-Q1.csv";
	private static String intialInputFileString = "C:\\Users\\hp\\Desktop\\Courses\\Team Project\\TeamProjectDemo1\\CSV2\\"+fileName;
	private static String file1RapidMinerProcessLink = "//Local Repository/processes/TermCount";
	private static String rapidMinerGeneratedFileLink = "C:\\Users\\hp\\Desktop\\Courses\\Team Project\\TeamProjectDemo1\\TeamDemo1.csv";
	private static String finalCSVFileSavingLink = "C:\\Users\\hp\\Desktop\\Courses\\Team Project\\TeamProjectDemo1\\Result -CSV1- Omer\\result"+fileName;
	
	public static StringBuilder stringBuilder = new StringBuilder();
	public static ArrayList<List<String>> csvData= new ArrayList<List<String>>();//data in columns in csv	
	public static ArrayList<List<String>> phrasesTokens= new ArrayList<List<String>>();//data about which tokens are contained in which phrases
	public static ArrayList<List<String>> termsInDocs= new ArrayList<List<String>>();//counts of terms in docs	
	
	private int numberOfPagesToCrawl = 5;
	private int documentSavedIndex = 0;

	public static void preprocesPhrase(String phrase){
		// creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution 
		List<String> additList= new ArrayList<String>();
		Properties props = new Properties();
		props.setProperty("annotators", "tokenize, ssplit, pos, lemma, ner, parse, dcoref");
		StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

		// create an empty Annotation just with the given text
		Annotation document = new Annotation(phrase);

		// run all Annotators on this text
		pipeline.annotate(document);
	
		// these are all the sentences in this document
		// a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
		List<CoreMap> sentences = document.get(SentencesAnnotation.class);
		
		for(CoreMap sentence: sentences) {
		  // traversing the words in the current sentence
		  // a CoreLabel is a CoreMap with additional token-specific methods
		  for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
		    // this is the text of the token
		    String word = token.get(TextAnnotation.class);
		    // this is the POS tag of the token
		    String pos = token.get(PartOfSpeechAnnotation.class);
		    // this is the NER label of the token
		    String ne = token.get(NamedEntityTagAnnotation.class);
		    
		    System.out.println("Word:("+word+") POS("+pos+") EntityName("+ne+")"+"\n");

			 if( !pos.equalsIgnoreCase("CC")&&
					 !pos.equalsIgnoreCase("CD")&&
					 !pos.equalsIgnoreCase("DT")&&
					 !pos.equalsIgnoreCase("EX")&&
					 !pos.equalsIgnoreCase("FW")&&
					 !pos.equalsIgnoreCase("IN")&&
					 !pos.equalsIgnoreCase("LS")&&
					 !pos.equalsIgnoreCase("MD")&&
					 !pos.equalsIgnoreCase("PDT")&&
					 !pos.equalsIgnoreCase("POS")&&
					 !pos.equalsIgnoreCase("PRP")&&
					 !pos.equalsIgnoreCase("PRP$")&&
					 !pos.equalsIgnoreCase("RP")&&
					 !pos.equalsIgnoreCase("SYM")&&
					 !pos.equalsIgnoreCase("UH")&&
					 !pos.equalsIgnoreCase("TO")&& 
					 !pos.equalsIgnoreCase("WDT")&& 	 
					 !pos.equalsIgnoreCase("WP")&&
					 !pos.equalsIgnoreCase("WP$")&&
					 !pos.equalsIgnoreCase("WRB")&&
					 !pos.equalsIgnoreCase("``")&&
					 !word.contains(",")&&
					 !word.contains("?")&&
					 !word.contains("|")){
				 	additList.add(word);
				 	if(!stringBuilder.toString().contains(word)){
				 		word = word.toLowerCase();
				 		stringBuilder.append(word+" ");
				 	}
		    	}
			 
			
           }//check if the word does not exit in the string
		
		  phrasesTokens.add(additList);
		  // this is the parse tree of the current sentence
		  Tree tree = sentence.get(TreeAnnotation.class);
		}	
	}

	public static List<String> csvFilePhrases(){
		// This function should be modified to load all csv files
		String csvFile = intialInputFileString;//"C:\\Users\\Àëåêñàíäðà\\Desktop\\Mannheim\\2s\\teampr\\Clustered Questions\\Topic 1-Q1.csv";
    	BufferedReader input = null;
		List<String> phrasesToSend = new ArrayList<String>();
		try 
		{
		    input =  new BufferedReader(new FileReader(csvFile));
		    String line = null; boolean firstLine = true;
		    while (( line = input.readLine()) != null)
		    {  if(!firstLine)
		    {
		    String [] data = line.split("\\|");// change it to | instead of ;
		     csvData.add(Arrays.asList(data));}
		    else{firstLine = false;}
		    }
		    }
		catch (Exception ex)
		{
		      ex.printStackTrace();
		}
		finally 
		{
		    if(input != null)
		    {
		    	try {input.close();} catch (IOException e) {e.printStackTrace();}
		    }
		}
		for (int i=0;i<csvData.size();i++)
		{phrasesToSend.add((csvData.get(i)).get(0));
		}
		
		return phrasesToSend;
	}
	
	public static void csvTermsInDocs(){
		String csvFile = rapidMinerGeneratedFileLink;//"C:\\Users\\Àëåêñàíäðà\\Desktop\\1.csv";
		BufferedReader input = null;
		try 
		{
		    input =  new BufferedReader(new FileReader(csvFile));
		    String line = null; boolean firstLine = true;
		    while (( line = input.readLine()) != null)
		    { 
		    if(!firstLine)
		    {String [] data = line.split(";");
		    termsInDocs.add(Arrays.asList(data));}
		    else{firstLine = false;
		    String [] data = line.split(";");
		   for (int i=0;i<data.length;i++){
		   data[i]=  data[i].substring(1, data[i].length()-1);
		   } termsInDocs.add(Arrays.asList(data));
		    }}
		}
		catch (Exception ex)
		{
		      ex.printStackTrace();
		}
		finally 
		{
		    if(input != null)
		    {
		    	try {
		    		input.close();
		    	} catch (IOException e){
		    		e.printStackTrace();
		    		}
		    }
		}
	}
	
	

      public static void writeCsvFile(String fileName,ArrayList<double[]> countsInDocs) {
		    
    	    String header = "phrase;"; //CSV file header
    	 	for(int i=0;i<csvData.get(1).size()-1;i++)
    	 	{header+="cluster"+Integer.toString(i+1)+";";}
    	 	for (int i=0;i<countsInDocs.get(0).length;i++)
    	 	{header+="F"+Integer.toString(i+1);
    	 	if(i!=countsInDocs.get(0).length-1){header+=";";}
    	 	}
    		
		    String delimiter = ";";//Delimiter used in CSV file
		    String separator = "\n";
		
	    	FileWriter fileWriter = null;
		        try {
	            fileWriter = new FileWriter(fileName); //Write the CSV file header
                fileWriter.append(header.toString()); //Add a new line separator after the header
                fileWriter.append(separator); //Write a new student object list to the CSV file
                System.out.println(csvData.size()+" "+countsInDocs.size());
     	 
                for(int i=0;i<csvData.size();i++)
     		      {fileWriter.append(Integer.toString(i+1)); fileWriter.append(';');
                	for(int j=1;j<csvData.get(i).size();j++)
     		         { fileWriter.append(csvData.get(i).get(j)); fileWriter.append(';'); }
     		       for(int j=0;j<countsInDocs.get(i).length;j++)
   		          { fileWriter.append(String.valueOf(countsInDocs.get(i)[j]));
   		           if(j!=countsInDocs.get(0).length-1) {fileWriter.append(delimiter);}
   		            else{ fileWriter.append(separator);} 
   		          }
     			}
	        System.out.println("CSV file was created successfully !!!");

	        } catch (Exception e) {
	
	            System.out.println("Error in CsvFileWriter !!!");
	
	            e.printStackTrace();

	        } finally {
	        try {
	        fileWriter.flush();
	             fileWriter.close();
	          } catch (IOException e) {
	             System.out.println("Error while flushing/closing fileWriter !!!");
	                e.printStackTrace();
	            }
	        }
	    }

  	//get Data From Galago and Save Document
  	
  	public static void getFilesFromGilagoAndSaveThem(String inputString) throws IOException{
  		
  		Document doc = null;
  		//open Galago URL with Query

  			//add + sign in empty string for URL generations
  	     	inputString = inputString.replace(" ", "+");
  			inputString = inputString.substring(0, inputString.length()-1);
  			StringBuilder urlInputString = new StringBuilder();
  			urlInputString.append("http://192.168.56.1:3002/search?q=%23sdm%28");//urlInputString.append("http://192.168.56.1:3002/search?q=omer&start=0&n=60");//
  			urlInputString.append(inputString);
  			urlInputString.append("%29&start=0&n=60");//urlInputString.append("%29&start=0&n=15");
  			
  			
  			doc = load_doc_from_url(urlInputString.toString());			
  			
  	  		if(doc != null){
  	  		//get URL Elements
  	  	  		Elements links = doc.select("a[href]");
  	  	  		
  	  	  		//Next Tasks 
  	  	  		//get documents 
  	  	  		//store them 
  	  	  		int index = 0;
  	  	  		for(Element element: links){
  	  	  			String parsedNextpargeURL = element.toString().substring(element.toString().indexOf("href=\"")+6, element.toString().indexOf("\">"));
  	  	  			if(parsedNextpargeURL.contains("document")){
  	  	  				// Request Pages of next documents
  	  	  				Document localdoc = load_doc_from_url("http://192.168.56.1:3002/"+parsedNextpargeURL);
  	  	  				
  	  	  				if(localdoc != null && index <10){
  	  	  				System.out.print(localdoc+"\n");
	  	  	  				System.out.print("**************************");
	  	  	  				System.out.print("**************************");
	  	  	  				saveDocument(localdoc,index);
	  	  	  				index++;
  	  	  				}
  	  	  			
  	  	  				//Learn endangered species area protect natural life environment Visit national wildlife refuge , park other open space ? courage everyone be friendly Make home plant more native plants donØºt use Herbicides pesticides trees Slow down driving hit animals road Recycle buy sustainable products Protect habitat harass flowers local"
  	  	  			}else if(parsedNextpargeURL.contains("search")){
  	  	  				
  	  	  			}
  	  	  		}
  	  		}
  	}
  	
	//save Documents give the page to URL of document
	void save10DocumentsWithURL(Elements links) throws IOException{
  		for(Element element: links){
  			String parsedNextpargeURL = element.toString().substring(element.toString().indexOf("href=\"")+6, element.toString().indexOf("\">"));
  			if(parsedNextpargeURL.contains("document")){
  				// Request Pages of next documents
  				Document localdoc = load_doc_from_url("http://192.168.56.1:3002/"+parsedNextpargeURL);
  				saveDocument(localdoc,documentSavedIndex);
  				documentSavedIndex++;
  				//Learn endangered species area protect natural life environment Visit national wildlife refuge , park other open space ? courage everyone be friendly Make home plant more native plants donØºt use Herbicides pesticides trees Slow down driving hit animals road Recycle buy sustainable products Protect habitat harass flowers local"
  			}
  		}
	}
  	
	//crawl 50 pages recursive funtions which gets called for crawling purpose
  	void crawl50Pages(String startingLink) throws IOException{
  		//load intial document
  		Document localdoc = load_doc_from_url("http://192.168.56.1:3002/"+startingLink);
  		//save 10 documents
  		save10DocumentsWithURL(localdoc.select("a[href]"));
  		//iterate to next value
  		if (numberOfPagesToCrawl>0) {
			crawl50Pages("a");
		}
  	}
    
	/**
	 * Load a url and return the parsed html document
	 * 
	 * @param url	an absolute url giving the base location of a page 
	 * @return a Jsoup Document representing the website behind the url
	 * @throws IOException
	 */
	public static Document load_doc_from_url(String url) throws IOException {
		//TODO 1 create correct return value
		 try {
				Document document = Jsoup.connect(url).timeout(10000000).get();
				return document;
			 
		    } catch (Exception exception) {
		    	return null;
		    }
	}
	
	//save document locally
	public static void saveDocument(Document docToSave, int index) throws IOException{
		
		final File file = new File("TeamFiles\\"+index+".text");
		BufferedWriter out = new BufferedWriter(new FileWriter(file));
		/*
		 * Stanford Lemitizer applying on saving documents 
		 * */
		StanfordLemmatizer stanfordLemmatizer = new StanfordLemmatizer();		
		out.write(stanfordLemmatizer.lemmatizeString(Jsoup.parse(docToSave.html()).text()));
		out.close();
	}
	
	//run Rapid Miner Process
	public static void runRapidMinerProcess() throws RepositoryException, IOException, XMLException, OperatorException{
		
	   	// this initializes RapidMiner with your repositories available
    	// you have to call this for all your programs, otherwise your RapidMiner integration will not work correctly
    	RapidMiner.setExecutionMode(RapidMiner.ExecutionMode.COMMAND_LINE);
    	RapidMiner.init();

    	// loads the process from the repository (if you do not have one, see alternative below)
    	RepositoryLocation pLoc = new RepositoryLocation(file1RapidMinerProcessLink);
    	ProcessEntry pEntry = (ProcessEntry) pLoc.locateEntry();
    	String processXML = pEntry.retrieveXML();
    	Process myProcess = new Process(processXML);
    	myProcess.setProcessLocation(new RepositoryProcessLocation(pLoc));
    	IOContainer ioResult = myProcess.run();
    	
	}
  	
	public static void main(String[] args) throws IOException, RepositoryException, XMLException, OperatorException {
		//parses csv files
	    List<String> phraseList = csvFilePhrases();			
		for (int i=0;i<csvData.size();i++)      //!!! look at the results some of the phrases are splitted into several as there is ',' in the phrase
		{for (int j=0;j<csvData.get(i).size();j++)
		{System.out.println((csvData.get(i)).get(j));}
		System.out.println("***");}			
		
		//preprocess phrases
	    for(int i=0; i< phraseList.size(); i++)
	          {System.out.println("************************\n");
					preprocesPhrase(phraseList.get(i));
			 System.out.println("************************\n");}
		System.out.println(stringBuilder.toString());
		
		/*
		 * Applying lemitization to the input String
		 * */
		StanfordLemmatizer stanfordLemmatizer = new StanfordLemmatizer();
		getFilesFromGilagoAndSaveThem(stanfordLemmatizer.lemmatizeString(stringBuilder.toString()));
		
				//String to send to Gilago
		System.out.println("************************\n");
		System.out.println("String to send to Gilago: "+stanfordLemmatizer.lemmatizeString(stringBuilder.toString()));
		System.out.println("************************\n");
		
		//Process CountTerms
		runRapidMinerProcess();

		//generates exception
		csvTermsInDocs();
		
		//save CSV file
		ArrayList<double[]> countsInDocs= new ArrayList<double[]>();//data in columns in csv	
		
		double[] add;
		for (int i=0;i<phrasesTokens.size();i++)      //!!! look at the results some of the phrases are splitted into several as there is ',' in the phrase
		{add=new double[termsInDocs.size()-1];
			for (int j=0;j<phrasesTokens.get(i).size();j++)
		{ 
			for (int k=0;k<termsInDocs.get(0).size()-4;k++)     
			{	
				
			if(((termsInDocs.get(0)).get(k).toLowerCase().contains((phrasesTokens.get(i)).get(j).toLowerCase()) && (termsInDocs.get(0)).get(k).length()-(phrasesTokens.get(i)).get(j).length()<=3)||((termsInDocs.get(0)).get(k).toLowerCase().equals((phrasesTokens.get(i)).get(j).toLowerCase())))
			{System.out.println("Phrase "+i+":"+(termsInDocs.get(0)).get(k)+k);
			double[] use=new double[termsInDocs.size()-1];
			for (int g=0;g<use.length;g++)
			{use[g]=Double.parseDouble((termsInDocs.get(g+1)).get(k));
			System.out.println(use[g]);
			add[g]+=use[g];
			}
			}
			}
		}
		countsInDocs.add(add);
		}		
		System.out.println(countsInDocs.get(0).length);
        writeCsvFile(finalCSVFileSavingLink, countsInDocs);
		
		}
	
	}
